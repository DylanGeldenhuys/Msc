{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import matplotlib.image as mpimg\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import io, transform\n",
    "from math import *\n",
    "import xml.etree.ElementTree as ET \n",
    "import pandas as pd\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from skimage.transform import rotate as rotate_transform\n",
    "from skimage.util import random_noise\n",
    "from skimage.filters import gaussian\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Transforms():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def rotate(self, image, params):\n",
    "\n",
    "        angle = params['rotation_range'][0]\n",
    "        angle = (random.uniform(0,1))*random.choice([-1,1])*angle\n",
    "        transformation_matrix = torch.tensor([\n",
    "            [+cos(radians(angle)), -sin(radians(angle))], \n",
    "            [+sin(radians(angle)), +cos(radians(angle))]\n",
    "        ])\n",
    "\n",
    "        image = rotate_transform(np.array(image), angle = angle, mode = 'edge')\n",
    "\n",
    "        # PIL expects RGB images to be uint with ranges from 0 to 255 so we have to convert it to a type that PIL can excpect ie a uint from 0 to 255 \n",
    "        return Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "    def translation(self, image,  params):\n",
    "        image_shape = np.array(image).shape\n",
    "        ty = random.uniform(params['height_shift_range'][0]*image_shape[0],          \n",
    "                            params['height_shift_range'][1]*image_shape[0])\n",
    "        tx = random.uniform(params['width_shift_range'][0]*image_shape[1],\n",
    "                            params['width_shift_range'][1]*image_shape[1] )\n",
    "\n",
    "        \n",
    "        horizontal_shift =  tx*random.choice([-1,1])\n",
    "        vertical_shift = ty*random.choice([-1,1])\n",
    "        horizontal_shift_normalised = horizontal_shift/image_shape[1]\n",
    "        vertical_shift_normalised =  vertical_shift/image_shape[0]\n",
    "\n",
    "        transform = AffineTransform(translation=(-horizontal_shift,-vertical_shift))\n",
    "\n",
    "        image = warp(np.array(image),transform,mode='edge')\n",
    "\n",
    "\n",
    "  \n",
    "        # PIL expects RGB images to be uint with ranges from 0 to 255 so we have to convert it to a type that PIL can excpect ie a uint from 0 to 255 \n",
    "        return Image.fromarray((image * 255).astype(np.uint8))\n",
    "        \n",
    "    def resize(self, image, img_size):\n",
    "        image = TF.resize(image, img_size)\n",
    "        return image\n",
    "\n",
    "    def zoom(self, image, params):\n",
    "\n",
    "        img_shape = np.array(image).shape\n",
    "        zoom = random.uniform(params['zoom_range'][0],params['zoom_range'][1])\n",
    "        image = TF.resize(image,(int(img_shape[0]*zoom), int(img_shape[1]*zoom)) )\n",
    "        scale_transform = torch.tensor([[zoom, 0], \n",
    "                                        [0, zoom]])\n",
    "\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def color_jitter(self, image):\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3, \n",
    "                                              contrast=0.3,\n",
    "                                              saturation=0.3, \n",
    "                                              hue=0.1)\n",
    "        image = color_jitter(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __call__(self, image, params, image_size):\n",
    "\n",
    "        # set checked image and landmark to landmark_ and image_ (this is for making sure we use the last checked tranformed instead of wrongly tranformed to do the following               # tranform)\n",
    "        \n",
    "        # -----------------------\n",
    "        image_ = Image.fromarray(image.copy())\n",
    "\n",
    "        # -----------------------\n",
    "\n",
    "        # ZOOM\n",
    "        image  = self.zoom(image_,  params)\n",
    "        \n",
    "\n",
    "        # RESIZE\n",
    "\n",
    "        image = self.resize(image, (image_size, image_size))\n",
    "\n",
    "        # ----------------------\n",
    "        #image_, landmarks_ = self.color_jitter(image_, landmarks_)\n",
    "        # ----------------------\n",
    "        \n",
    "        # ROTATE\n",
    "        image = self.rotate(image,  params)\n",
    "\n",
    "\n",
    "        # ----------------------\n",
    "\n",
    "        image = image\n",
    "        # ----------------------\n",
    "\n",
    "        # TRANSLATION\n",
    "        image= self.translation(image, params)\n",
    "\n",
    " \n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        # the following tranform normalises each channel to have a mean at 0.5 and std of 0.5 / NOTE: NOT sure if this is theoreticlly better, should check this\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        return image\n",
    "\n",
    "class LandmarksDataset():\n",
    "\n",
    "    def __init__(self, transform=None,zoom = [1.0 - 0.03258157476873315, 1.0 + 0.03258157476873315], rotation = [22], height_shift= [0,0.03003200603616672], width_shift= [0,0.03003200603616672 ]):\n",
    "\n",
    "        # targets 0\n",
    "        filenames1 = os.listdir('D:/Tsetse fly Project/Data/Missing_landmarkwings_L/')\n",
    "        filenames2 = os.listdir('D:/Tsetse fly Project/Data/Missing_landmarkwings_R/')\n",
    "        # targets 1\n",
    "        filenames3 = os.listdir('D:/Tsetse fly Project/Data/tsetsedata_2019_left_commas/images_left/')\n",
    "    \n",
    "        self. tranform = transform\n",
    "        self.zoom = zoom\n",
    "        self.rotation = rotation\n",
    "        self.height_shift = height_shift\n",
    "        self.width_shift = width_shift\n",
    "        self.image_filenames = []\n",
    "        self.targets = []\n",
    "        self.image_size = 244\n",
    "        self.transform = transform\n",
    "        self.image_dir = 'D:/Tsetse fly Project/Data/Missing_landmarkwings_L/'\n",
    "        \n",
    "        self.image_dir2 = 'D:/Tsetse fly Project/Data/Missing_landmarkwings_R/'\n",
    "        self.image_dir3 = 'D:/Tsetse fly Project/Data/tsetsedata_2019_left_commas/images_left/'\n",
    "        self.TransF_ = True\n",
    "\n",
    "       # ------------------- Append left wings data to dataset class ------------\n",
    "\n",
    "        for filename in filenames1[:]:\n",
    "            self.image_filenames.append(os.path.join(self.image_dir, filename))\n",
    "            self.targets.append(1)\n",
    "\n",
    "            \n",
    "\n",
    "        # ------------------ Append flipped right wings data to dataset class-----\n",
    "\n",
    "\n",
    "        for filename in filenames2[:]:\n",
    "            self.targets.append(1)\n",
    "            self.image_filenames.append(os.path.join(self.image_dir2, filename))\n",
    "\n",
    "        num = len(self.targets.copy())\n",
    "        for filename in filenames3[:num]:\n",
    "            self.targets.append(0)\n",
    "            self.image_filenames.append(os.path.join(self.image_dir3, filename))\n",
    "\n",
    "\n",
    "\n",
    "        # ----------------------\n",
    "\n",
    "    def TransF(self):\n",
    "        self.TransF_ = True\n",
    "    def NoTransF(self):\n",
    "        self.TransF_ = False\n",
    "    def resize(self,size):\n",
    "        self.image_size = size\n",
    "    def set_params(self, zoom = [0.95, 0.105], rotation = [10], height_shift= [0,0.05], width_shift= [0,0.05]):\n",
    "        self.zoom = zoom\n",
    "        self.rotation = rotation\n",
    "        self.height_shift = height_shift\n",
    "        self.width_shift = width_shift\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        params = {'zoom_range': self.zoom, 'rotation_range':self.rotation, 'height_shift_range': self.height_shift, 'width_shift_range': self.width_shift }\n",
    "        image_ = plt.imread(self.image_filenames[index])\n",
    "        target = torch.tensor(self.targets[index])\n",
    "\n",
    "        image = plt.imread(self.image_filenames[index])\n",
    "\n",
    "        \n",
    "        if self.transform and self.TransF_:\n",
    "            \n",
    "            image = self.transform(image_, params, self.image_size)\n",
    "\n",
    "        else:\n",
    "            img_shape = image.copy().shape\n",
    "            image = Image.fromarray(image)\n",
    "            image = TF.resize(image, (self.image_size,self.image_size))\n",
    "       \n",
    "            image = TF.to_tensor(image)\n",
    "            # the following tranform normalises each channel to have a mean at 0.5 and std of 0.5 / NOTE: NOT sure if this is theoreticlly better, should check this\n",
    "            image = TF.normalize(image, [0.5], [0.5])\n",
    "\n",
    "        return image, target, index#self.image_filenames[index]\n",
    "\n",
    "DataSet = LandmarksDataset(Transforms())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread('D:/Tsetse fly Project/Data/tsetsedata_2019_left_commas/images_left/'+ os.listdir(image_dir3)[0])\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nYou instantiate it with the same arguments used for the normalize. and then use it the same way\\n\\nunorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\\nunorm(tensor)\\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "\n",
    "\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "'''\n",
    "You instantiate it with the same arguments used for the normalize. and then use it the same way\n",
    "\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "unorm(tensor)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "\n",
    "class vgg16_bn_(nn.Module):\n",
    "    def __init__(self,num_classes=1):\n",
    "        super().__init__()\n",
    "        self.model_name='vgg16_bn'\n",
    "        self.model=models.vgg16_bn(pretrained=True)\n",
    "        #self.model.conv1=nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.classifier=nn.Linear(self.model.classifier[0].in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.model(x))\n",
    "        return x\n",
    "\n",
    "class resnet18_(nn.Module):\n",
    "    def __init__(self,num_classes=1):\n",
    "        super().__init__()\n",
    "        self.model_name='resnet18'\n",
    "        self.model=models.resnet18(pretrained = True)\n",
    "        self.model.conv1=nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=torch.sigmoid(self.model(x))\n",
    "        return x\n",
    "\n",
    "class inception_(nn.Module):\n",
    "    def __init__(self,num_classes=1):\n",
    "        super().__init__()\n",
    "        self.model_name='inception'\n",
    "        self.model=models.inception_v3(pretrained=True)\n",
    "        self.model.Conv2d_1a_3x3.conv=nn.Conv2d(3, 32, kernel_size=3, stride=2,  bias=False)\n",
    "        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        if self.model.training:\n",
    "        \n",
    "            x = torch.sigmoid(x.logits)\n",
    "        else:\n",
    "            \n",
    "            x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The length of Train set is 618\nThe length of Valid set is 205\nThe length of Valid set is 205\n"
     ]
    }
   ],
   "source": [
    "# split data for image size 244\n",
    "\n",
    "DataSet.NoTransF()\n",
    "DataSet.resize(244)\n",
    "dataset = DataSet\n",
    "# split the dataset into validation and test sets\n",
    "len_valid_test_set = int(0.2*len(dataset)) # 60% training, 20% validation, 20% testing\n",
    "\n",
    "len_train_set = len(dataset) - len_valid_test_set*2\n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_test_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_test_set))\n",
    "\n",
    "train_dataset , valid_dataset, test_dataset  = torch.utils.data.random_split(dataset , [len_train_set, len_valid_test_set, len_valid_test_set], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# shuffle and batch the datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=15, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "List has unique elements.\n"
     ]
    }
   ],
   "source": [
    "list1 = []\n",
    "for a,b,i in :\n",
    "    list1.append(i)\n",
    "list2=list(set(list1))\n",
    "if len(list2)==len(list1):\n",
    "    print('List has unique elements.')\n",
    "else:\n",
    "    print('List has duplicate elements.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load vgg16_bn and make prediction on test set\n",
    "\n",
    "vgg16 = vgg16_bn_()\n",
    "vgg16.load_state_dict(torch.load('C:/Users/dylan/Work-Projects/msc_haar/manuscript1_exp/models/model_vgg16_bn_classifer_finetune.pth'))\n",
    "vgg16.cpu()\n",
    "vgg16.eval()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "y_pred_vgg16 = np.array([])\n",
    "y_truth_vgg16 = np.array([])\n",
    "for images,targets in test_loader:\n",
    "    images = images.reshape((1,3,244,244))\n",
    "    y = vgg16(images)\n",
    "    y = y.detach().flatten().numpy()\n",
    "    y_pred_vgg16 = np.append(y_pred_vgg16, y)\n",
    "    y_truth_vgg16 = np.append(y_truth_vgg16, targets.detach().flatten().numpy())\n",
    "\n",
    "\n",
    "endtime = time.time()-start_time\n",
    "print('time vgg16',endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load resnet18 and make prediction on test set\n",
    "\n",
    "resnet18 = resnet18_()\n",
    "resnet18.load_state_dict(torch.load('C:/Users/dylan/Work-Projects/msc_haar/manuscript1_exp/models/model_resnet18_classifer_finetune2.pth'))\n",
    "resnet18.cpu()\n",
    "resnet18.eval()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "fp = np.array([])\n",
    "tp = np.array([])\n",
    "fn = np.array([])\n",
    "tn = np.array([])\n",
    "\n",
    "y_pred_resnet18 = np.array([])\n",
    "y_truth_resnet18 = np.array([])\n",
    "for images,targets, imagename in test_loader:\n",
    "    images = images.reshape((1,3,244,244))\n",
    "    y = resnet18(images)\n",
    "    y = y.detach().flatten().numpy()\n",
    "    y_pred_resnet18 = np.append(y_pred_resnet18, y)\n",
    "    y_truth_resnet18 = np.append(y_truth_resnet18, targets.detach().flatten().numpy())\n",
    "    \n",
    "    if y < 0.197434 :\n",
    "        y = 0\n",
    "    else:\n",
    "        y = 1\n",
    "\n",
    "    if round(y) == 1 and targets == 0:\n",
    "        fp = np.append(fp, imagename[-15:])\n",
    "    elif round(y) == 1 and targets == 1:\n",
    "        tp = np.append(tp, imagename[-15:])\n",
    "    elif round(y) == 0 and targets == 1:\n",
    "        fn = np.append(fn, imagename[-15:])\n",
    "    elif y == 0 and targets == 0:\n",
    "        tn = np.append(tn, imagename[-15:])\n",
    "    \n",
    "\n",
    "endtime = time.time()-start_time\n",
    "print('time resnet',endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [fp, tp, fn, tn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifdata = pd.DataFrame([fp, tp, fn, tn])\n",
    "\n",
    "classifdata = classifdata.T\n",
    "classifdata.columns = ['fp', 'tp', 'fn', 'tn']\n",
    "classifdata.to_csv('binaryclassifications.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images,targets, imagename in test_loader:\n",
    "    if imagename[-3:] == 'jpg':\n",
    "        print(imagename[-26:])\n",
    "        unorm = UnNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        image = unorm(images)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image)\n",
    "        plt.savefig('C:/Users/dylan/Work-Projects/msc_haar/manuscript1_exp/testset_classif_willieswings/{}'.format(imagename[-26:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload test data with the same seed with image size 299, Load Inception and make prediction on test set\n",
    "\n",
    "DataSet.NoTransF()\n",
    "DataSet.resize(299)\n",
    "dataset = DataSet\n",
    "# split the dataset into validation and test sets\n",
    "len_valid_test_set = int(0.2*len(dataset)) # 60% training, 20% validation, 20% testing\n",
    "\n",
    "len_train_set = len(dataset) - len_valid_test_set*2\n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_test_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_test_set))\n",
    "\n",
    "train_dataset , valid_dataset, test_dataset  = torch.utils.data.random_split(dataset , [len_train_set, len_valid_test_set, len_valid_test_set], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# shuffle and batch the datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=15, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=None, batch_sampler=None,  shuffle=False)\n",
    "\n",
    "print('evaluating')\n",
    "inception = inception_()\n",
    "inception.load_state_dict(torch.load('C:/Users/dylan/Work-Projects/msc_haar/manuscript1_exp/models/model_inception_classifer_finetune.pth'))\n",
    "inception.cpu()\n",
    "inception.eval()\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_inception = []\n",
    "y_truth_inception = []\n",
    "for images,targets in test_loader:\n",
    "    images = images.reshape((1,3,299,299))\n",
    "    y = inception(images)\n",
    "    y = y.detach().flatten().numpy()\n",
    "    y_pred_inception = np.append(y_pred_inception, y)\n",
    "    y_truth_inception = np.append(y_truth_inception, targets.detach().flatten().numpy())\n",
    "\n",
    "endtime = time.time()-start_time\n",
    "print('time inception',endtime)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for vgg16_bn\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#\n",
    "print('vgg16')\n",
    "vggpred = []\n",
    "for i in y_pred_vgg16:\n",
    "    if i<0.5:\n",
    "        vggpred.append(0)\n",
    "    else:\n",
    "        vggpred.append(1)\n",
    "\n",
    "print(classification_report(y_truth_vgg16, vggpred))\n",
    "print('AUC ROC:', roc_auc_score(y_truth_vgg16, y_pred_vgg16))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_truth_vgg16, vggpred)\n",
    "row_sums = cm.sum(axis=1)\n",
    "new_matrix = cm / row_sums[:, np.newaxis]\n",
    "display(new_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('resnet18')\n",
    "resnet18pred = []\n",
    "for i in y_pred_resnet18:\n",
    "    if i<0.23:\n",
    "        resnet18pred.append(0)\n",
    "    else:\n",
    "        resnet18pred.append(1)\n",
    "\n",
    "print(classification_report(y_truth_resnet18, resnet18pred))\n",
    "print('AUC ROC:', roc_auc_score(y_truth_resnet18, y_pred_resnet18))\n",
    "\n",
    "cm = confusion_matrix(y_truth_resnet18, resnet18pred)\n",
    "row_sums = cm.sum(axis=1)\n",
    "new_matrix = cm / row_sums[:, np.newaxis]\n",
    "display(new_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('inception')\n",
    "inceptionpred = []\n",
    "for i in y_pred_inception:\n",
    "    if i<0.5:\n",
    "        inceptionpred.append(0)\n",
    "    else:\n",
    "        inceptionpred.append(1)\n",
    "\n",
    "print(classification_report(y_truth_inception, inceptionpred))\n",
    "print('AUC ROC:', roc_auc_score(y_truth_inception, y_pred_inception))\n",
    "\n",
    "cm = confusion_matrix( y_truth_inception, inceptionpred )\n",
    "row_sums = cm.sum(axis=1)\n",
    "new_matrix = cm / row_sums[:, np.newaxis]\n",
    "display(new_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth_vgg16, y_pred_vgg16)\n",
    "\n",
    "import pandas \n",
    "\n",
    "vgg16 = pd.DataFrame([fpr, tpr, thresholds])\n",
    "vgg = vgg16.T.iloc[9:13, :]\n",
    "vgg.columns = ['fpr', 'tpr', 'thresholds']\n",
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth_inception, y_pred_inception)\n",
    "\n",
    "import pandas \n",
    "\n",
    "vgg16 = pd.DataFrame([fpr, tpr, thresholds])\n",
    "vgg = vgg16.T.iloc[6:10, :]\n",
    "vgg.columns = ['fpr', 'tpr', 'thresholds']\n",
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth_resnet18, y_pred_resnet18)\n",
    "\n",
    "import pandas \n",
    "\n",
    "vgg16 = pd.DataFrame([fpr, tpr, thresholds])\n",
    "vgg = vgg16.T.iloc[6:9, :]\n",
    "vgg.columns = ['fpr', 'tpr', 'thresholds']\n",
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vgg16')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth_vgg16, y_pred_vgg16)\n",
    "\n",
    "# %%\n",
    "#plt.figure(figsize=(10,10))\n",
    "#\n",
    "plt.subplots(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, 'o-', label=\"ROC curve\", linewidth = 1)\n",
    "#plt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\n",
    "for x, y, txt in zip(fpr, tpr, thresholds):\n",
    "    plt.annotate(np.round(txt,2), (x, y))\n",
    "#plt.set_fontsize(10)\n",
    "rnd_idx = 12\n",
    "plt.annotate('threshold at 0.5', xy = (0.02631579, 0.97368), arrowprops=dict(facecolor='black', lw=2, arrowstyle='->'), xytext = (0.02631579-0.02, 0.9736+0.04))\n",
    "plt.scatter(0.02631579, 0.9736, color = 'r')\n",
    "#plt.annotate('this point refers to the tpr and the fpr\\n at a probability threshold of {}'.format(np.round(thresholds[rnd_idx], 2)), \n",
    "#             xy=(fpr[rnd_idx], tpr[rnd_idx]), xytext=(fpr[rnd_idx]+0.2, tpr[rnd_idx]-0.25),\n",
    "#             arrowprops=dict(facecolor='black', lw=2, arrowstyle='->'),)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('vgg16_bn ROC curve')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim((0,0.2))\n",
    "plt.ylim((0.8, 1.1))\n",
    "plt.savefig('vggroc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('inception')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth_inception, y_pred_inception)\n",
    "\n",
    "# %%\n",
    "plt.subplots(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, 'o-', label=\"ROC curve\")\n",
    "plt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\n",
    "for x, y, txt in zip(fpr[::5], tpr[::5], thresholds[::5]):\n",
    "    plt.annotate(np.round(txt,2), (x, y-0.04))\n",
    "rnd_idx = 7\n",
    "plt.annotate('this point refers to the tpr and the fpr\\n at a probability threshold of {}'.format(np.round(thresholds[rnd_idx], 2)), \n",
    "             xy=(fpr[rnd_idx], tpr[rnd_idx]), xytext=(fpr[rnd_idx]+0.2, tpr[rnd_idx]-0.25),\n",
    "             arrowprops=dict(facecolor='black', lw=2, arrowstyle='->'),)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('Inception ROC curve')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.savefig('inceproc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('inception')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth_inception, y_pred_inception)\n",
    "\n",
    "# %%\n",
    "plt.subplots(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, 'o-', label=\"ROC curve\")\n",
    "plt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\n",
    "for x, y, txt in zip(fpr, tpr, thresholds):\n",
    "    plt.annotate(np.round(txt,2), (x, y))\n",
    "#0.97368421, 0.02631579\n",
    "plt.annotate('threshold at 0.5', xy = (0.02631579,0.974), arrowprops=dict(facecolor='black', lw=2, arrowstyle='->'), xytext = (0.02631579-0.02, 0.97368421+0.04))\n",
    "plt.scatter(0.02631579,0.979, color = 'r')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('Inception ROC curve')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim((0,0.2))\n",
    "plt.ylim((0.8, 1.1))\n",
    "plt.savefig('inceproc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('resnet18')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth_resnet18, y_pred_resnet18)\n",
    "\n",
    "# %%\n",
    "plt.subplots(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, 'o-', label=\"ROC curve\")\n",
    "plt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\n",
    "for x, y, txt in zip(fpr[::5], tpr[::5], thresholds[::5]):\n",
    "    plt.annotate(np.round(txt,2), (x, y-0.04))\n",
    "rnd_idx = 6\n",
    "plt.annotate('this point refers to the tpr and the fpr\\n at a probability threshold of {}'.format(np.round(thresholds[rnd_idx], 2)), \n",
    "             xy=(fpr[rnd_idx], tpr[rnd_idx]), xytext=(fpr[rnd_idx]+0.2, tpr[rnd_idx]-0.25),\n",
    "             arrowprops=dict(facecolor='black', lw=2, arrowstyle='->'),)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('resnet18 ROC curve')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.savefig('resnetroc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('resnet18')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth_resnet18, y_pred_resnet18)\n",
    "\n",
    "# %%\n",
    "plt.subplots(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, 'o-', label=\"ROC curve\")\n",
    "plt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\n",
    "for x, y, txt in zip(fpr, tpr, thresholds):\n",
    "    plt.annotate(np.round(txt,2), (x, y))\n",
    "\n",
    "plt.annotate('threshold at 0.5', xy = ( 0.01754386,0.94174757), arrowprops=dict(facecolor='black', lw=2, arrowstyle='->'), xytext = ( 0.01754386-0.02, 0.94174757+0.04))\n",
    "plt.scatter( 0.01754386,0.94174757, color = 'r')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('resnet18 ROC curve')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim((-0.001,0.2))\n",
    "plt.ylim((0.8, 1.1))\n",
    "plt.savefig('resnetroc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model training loss')\n",
    "\n",
    "vgg17 = pickle.load( open( \"C:/Users/dylan/Work-Projects/msc_haar/manuscript1_exp/classifiers/model_vgg16_bn_classifer_finetune_trainingdata.pkl\", \"rb\" ) )\n",
    "resnet18 = pickle.load(open( \"C:/Users/dylan/Work-Projects/msc_haar/manuscript1_exp/classifiers/model_resnet18_classifer_finetune_trainingdata.pkl\", \"rb\" ))\n",
    "inception = pickle.load(open( \"C:/Users/dylan/Work-Projects/msc_haar/manuscript1_exp/classifiers/model_inception_classifer_finetune_trainingdata.pkl\", \"rb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(vgg17['acc_val'], label='vgg16_bn')\n",
    "plt.plot(resnet18['acc_val'], label='resnet18')\n",
    "plt.plot(inception['acc_val'], label='inception')\n",
    "plt.title('Validation accuracies')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('valepoch.png')\n",
    "plt.show()\n",
    "print(max(vgg17['acc_val']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vgg17['loss_val'], label='vgg16_bn')\n",
    "plt.plot(resnet18['loss_val'], label='resnet18')\n",
    "plt.plot(inception['loss_val'], label='inception')\n",
    "plt.title('Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "1. create a roc curve for all 3 models\n",
    "\n",
    "2. set up pipline "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}